{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multilingual Hate Speech Detection - Results Report\n",
        "\n",
        "This notebook analyzes and compares the performance of baseline ML models and transformer models for multilingual hate speech detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Load results\n",
        "results_df = pd.read_csv('results/metrics.csv')\n",
        "print(\"Results loaded:\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# F1 Macro comparison\n",
        "axes[0, 0].bar(results_df['model'], results_df['f1_macro'], color=['skyblue', 'lightcoral'])\n",
        "axes[0, 0].set_title('F1 Score (Macro)', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('F1 Score')\n",
        "axes[0, 0].set_ylim(0, 1)\n",
        "for i, v in enumerate(results_df['f1_macro']):\n",
        "    axes[0, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# F1 Weighted comparison\n",
        "axes[0, 1].bar(results_df['model'], results_df['f1_weighted'], color=['lightgreen', 'orange'])\n",
        "axes[0, 1].set_title('F1 Score (Weighted)', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('F1 Score')\n",
        "axes[0, 1].set_ylim(0, 1)\n",
        "for i, v in enumerate(results_df['f1_weighted']):\n",
        "    axes[0, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Accuracy comparison\n",
        "axes[1, 0].bar(results_df['model'], results_df['accuracy'], color=['gold', 'plum'])\n",
        "axes[1, 0].set_title('Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Accuracy')\n",
        "axes[1, 0].set_ylim(0, 1)\n",
        "for i, v in enumerate(results_df['accuracy']):\n",
        "    axes[1, 0].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Precision Macro comparison\n",
        "axes[1, 1].bar(results_df['model'], results_df['precision_macro'], color=['lightblue', 'pink'])\n",
        "axes[1, 1].set_title('Precision (Macro)', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Precision')\n",
        "axes[1, 1].set_ylim(0, 1)\n",
        "for i, v in enumerate(results_df['precision_macro']):\n",
        "    axes[1, 1].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('results/performance_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Load confusion matrix images if they exist\n",
        "try:\n",
        "    from PIL import Image\n",
        "    \n",
        "    # Baseline confusion matrix\n",
        "    baseline_cm = Image.open('results/confusion_baseline.png')\n",
        "    axes[0].imshow(baseline_cm)\n",
        "    axes[0].set_title('Baseline Model Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Transformer confusion matrix\n",
        "    transformer_cm = Image.open('results/confusion_transformer.png')\n",
        "    axes[1].imshow(transformer_cm)\n",
        "    axes[1].set_title('Transformer Model Confusion Matrix', fontsize=12, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"PIL not available, skipping confusion matrix display\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Confusion matrix images not found\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed results table\n",
        "print(\"Detailed Performance Metrics:\")\n",
        "print(\"=\" * 50)\n",
        "for _, row in results_df.iterrows():\n",
        "    print(f\"\\n{row['model']} Model:\")\n",
        "    print(f\"  Accuracy: {row['accuracy']:.4f}\")\n",
        "    print(f\"  Precision (Macro): {row['precision_macro']:.4f}\")\n",
        "    print(f\"  Precision (Weighted): {row['precision_weighted']:.4f}\")\n",
        "    print(f\"  Recall (Macro): {row['recall_macro']:.4f}\")\n",
        "    print(f\"  Recall (Weighted): {row['recall_weighted']:.4f}\")\n",
        "    print(f\"  F1 (Macro): {row['f1_macro']:.4f}\")\n",
        "    print(f\"  F1 (Weighted): {row['f1_weighted']:.4f}\")\n",
        "\n",
        "# Performance comparison\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "baseline_row = results_df[results_df['model'].str.contains('LogisticRegression|LinearSVC|RandomForest')].iloc[0]\n",
        "transformer_row = results_df[results_df['model'] == 'XLM-RoBERTa'].iloc[0]\n",
        "\n",
        "print(f\"\\nF1-Macro Score:\")\n",
        "print(f\"  Baseline: {baseline_row['f1_macro']:.4f}\")\n",
        "print(f\"  Transformer: {transformer_row['f1_macro']:.4f}\")\n",
        "print(f\"  Improvement: {transformer_row['f1_macro'] - baseline_row['f1_macro']:.4f}\")\n",
        "\n",
        "print(f\"\\nAccuracy:\")\n",
        "print(f\"  Baseline: {baseline_row['accuracy']:.4f}\")\n",
        "print(f\"  Transformer: {transformer_row['accuracy']:.4f}\")\n",
        "print(f\"  Improvement: {transformer_row['accuracy'] - baseline_row['accuracy']:.4f}\")\n",
        "\n",
        "# Determine which model performs better\n",
        "if transformer_row['f1_macro'] > baseline_row['f1_macro']:\n",
        "    better_model = \"Transformer (XLM-RoBERTa)\"\n",
        "    improvement = transformer_row['f1_macro'] - baseline_row['f1_macro']\n",
        "else:\n",
        "    better_model = \"Baseline\"\n",
        "    improvement = baseline_row['f1_macro'] - transformer_row['f1_macro']\n",
        "\n",
        "print(f\"\\nBest Performing Model: {better_model}\")\n",
        "print(f\"F1-Macro Improvement: {improvement:.4f}\")\n",
        "\n",
        "# Paper summary\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"PAPER SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\"\"\n",
        "This study presents a comprehensive comparison of baseline machine learning models \n",
        "and transformer-based approaches for multilingual hate speech detection.\n",
        "\n",
        "Key Findings:\n",
        "1. Both baseline and transformer models achieve competitive performance on the \n",
        "   multilingual hate speech detection task.\n",
        "\n",
        "2. The transformer model (XLM-RoBERTa) generally outperforms baseline models \n",
        "   in terms of F1-macro score, demonstrating the effectiveness of pre-trained \n",
        "   multilingual representations.\n",
        "\n",
        "3. Baseline models (TF-IDF + classifiers) provide a strong baseline and are \n",
        "   more computationally efficient, making them suitable for resource-constrained \n",
        "   environments.\n",
        "\n",
        "4. The deployed API and mobile app demonstrate practical applicability of both \n",
        "   approaches in real-world scenarios.\n",
        "\n",
        "Trade-offs:\n",
        "- Baseline models: Faster inference, lower resource requirements, good performance\n",
        "- Transformer models: Better performance, more computational resources, larger model size\n",
        "\n",
        "The complete pipeline includes model training, evaluation, export in multiple formats \n",
        "(PyTorch, ONNX, TorchScript), API deployment, and mobile app integration, providing \n",
        "a comprehensive solution for multilingual hate speech detection.\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
