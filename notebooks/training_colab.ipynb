{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multilingual Hate Speech Detection Training\n",
        "\n",
        "This notebook trains baseline ML models and transformer models for multilingual hate speech detection, then exports them for API and mobile deployment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install transformers datasets accelerate scikit-learn onnx onnxruntime torch matplotlib seaborn pandas numpy\n",
        "%pip install --upgrade huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
        ")\n",
        "from datasets import load_dataset, Dataset as HFDataset\n",
        "import onnx\n",
        "from onnxruntime import InferenceSession\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the multilingual hate speech dataset focusing on Arabic, Turkish, and English\n",
        "print(\"Loading multilingual dataset for Arabic, Turkish, and English...\")\n",
        "\n",
        "# Load the main training dataset\n",
        "train_dataset = load_dataset(\"FrancophonIA/multilingual-hatespeech-dataset\", \"Multilingual_train\")\n",
        "print(f\"Training dataset loaded: {len(train_dataset['train'])} samples\")\n",
        "\n",
        "# Load specific language test datasets\n",
        "target_languages = ['Arabic_test', 'Turkish_test', 'English_test']\n",
        "test_datasets = {}\n",
        "\n",
        "print(\"\\nLoading target language test datasets...\")\n",
        "for lang in target_languages:\n",
        "    try:\n",
        "        test_datasets[lang] = load_dataset(\"FrancophonIA/multilingual-hatespeech-dataset\", lang)\n",
        "        print(f\"‚úì {lang}: {len(test_datasets[lang]['test'])} samples\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó {lang}: Failed to load - {e}\")\n",
        "\n",
        "# Explore the main training dataset structure\n",
        "print(f\"\\nTraining dataset features:\")\n",
        "print(f\"Features: {train_dataset['train'].features}\")\n",
        "print(f\"Sample: {train_dataset['train'][0]}\")\n",
        "print(f\"Total training samples: {len(train_dataset['train'])}\")\n",
        "\n",
        "# Debug: Check the first few samples to understand the structure\n",
        "print(f\"\\nFirst 3 samples for debugging:\")\n",
        "for i in range(min(3, len(train_dataset['train']))):\n",
        "    sample = train_dataset['train'][i]\n",
        "    print(f\"Sample {i}: {sample}\")\n",
        "    print(f\"Keys: {list(sample.keys())}\")\n",
        "    print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_multilingual_text(text):\n",
        "    \"\"\"Enhanced preprocessing for Arabic, Turkish, and English text\"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to string and strip whitespace\n",
        "    text = str(text).strip()\n",
        "    \n",
        "    # Remove URLs\n",
        "    import re\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\\\s+', ' ', text)\n",
        "    \n",
        "    # Keep Arabic, Turkish, and English characters + basic punctuation\n",
        "    # Arabic: \\u0600-\\u06FF, Turkish: includes special chars like √ß, ƒü, ƒ±, √∂, ≈ü, √º\n",
        "    # English: a-zA-Z, numbers, and basic punctuation\n",
        "    text = re.sub(r'[^\\\\w\\\\s\\\\u0600-\\\\u06FF\\\\u00C0-\\\\u017F\\\\u0100-\\\\u017F\\\\u0180-\\\\u024F\\\\u1E00-\\\\u1EFF]', ' ', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "# Combine training data and preprocess\n",
        "all_texts = []\n",
        "all_labels = []\n",
        "\n",
        "# Process training data\n",
        "print(\"Processing training data...\")\n",
        "print(f\"Processing {len(train_dataset['train'])} training samples...\")\n",
        "\n",
        "for i, item in enumerate(train_dataset['train']):\n",
        "    if i < 5:  # Debug first few items\n",
        "        print(f\"Sample {i}: {item}\")\n",
        "    # Handle different possible label field names\n",
        "    label = None\n",
        "    if 'label' in item:\n",
        "        label = item['label']\n",
        "    elif 'labels' in item:\n",
        "        label = item['labels']\n",
        "    elif 'hate' in item:\n",
        "        label = item['hate']\n",
        "    \n",
        "    # Handle different possible text field names\n",
        "    text = None\n",
        "    if 'text' in item:\n",
        "        text = item['text']\n",
        "    elif 'content' in item:\n",
        "        text = item['content']\n",
        "    elif 'comment' in item:\n",
        "        text = item['comment']\n",
        "    \n",
        "    if text is not None and label is not None:\n",
        "        processed_text = preprocess_multilingual_text(text)\n",
        "        if processed_text:  # Only keep non-empty texts\n",
        "            all_texts.append(processed_text)\n",
        "            all_labels.append(label)\n",
        "\n",
        "print(f\"Processed {len(all_texts)} samples from training data\")\n",
        "print(f\"Sample processed text: {all_texts[0] if all_texts else 'No samples processed'}\")\n",
        "print(f\"Sample label: {all_labels[0] if all_labels else 'No labels processed'}\")\n",
        "\n",
        "# Optionally add some test data for more diverse training\n",
        "if test_datasets:\n",
        "    print(\"Adding test data for diversity...\")\n",
        "    test_samples_added = 0\n",
        "    for config_name, test_dataset in test_datasets.items():\n",
        "        if test_samples_added >= 500:  # Limit to avoid too much data\n",
        "            break\n",
        "        try:\n",
        "            for item in test_dataset['test'][:50]:  # Take up to 50 samples per language\n",
        "                label = None\n",
        "                if 'label' in item:\n",
        "                    label = item['label']\n",
        "                elif 'labels' in item:\n",
        "                    label = item['labels']\n",
        "                elif 'hate' in item:\n",
        "                    label = item['hate']\n",
        "                \n",
        "                text = None\n",
        "                if 'text' in item:\n",
        "                    text = item['text']\n",
        "                elif 'content' in item:\n",
        "                    text = item['content']\n",
        "                elif 'comment' in item:\n",
        "                    text = item['comment']\n",
        "                \n",
        "                if text is not None and label is not None:\n",
        "                    processed_text = preprocess_text(text)\n",
        "                    if processed_text:\n",
        "                        all_texts.append(processed_text)\n",
        "                        all_labels.append(label)\n",
        "                        test_samples_added += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {config_name}: {e}\")\n",
        "            continue\n",
        "    print(f\"Added {test_samples_added} additional samples from test datasets\")\n",
        "else:\n",
        "    print(\"No additional test datasets loaded, using only training data\")\n",
        "\n",
        "print(f\"Total samples after preprocessing: {len(all_texts)}\")\n",
        "print(f\"Label distribution: {np.bincount(all_labels)}\")\n",
        "\n",
        "# Check if we have any data\n",
        "if len(all_texts) == 0:\n",
        "    print(\"ERROR: No data processed! Check the dataset structure.\")\n",
        "    print(\"Available keys in first training sample:\", list(train_dataset['train'][0].keys()) if len(train_dataset['train']) > 0 else \"No training data\")\n",
        "    raise ValueError(\"No data was processed. Please check the dataset structure and field names.\")\n",
        "else:\n",
        "    print(f\"Sample texts: {all_texts[:3]}\")\n",
        "    print(f\"Sample labels: {all_labels[:3]}\")\n",
        "\n",
        "# Convert to binary classification (0: non-hate, 1: hate)\n",
        "binary_labels = []\n",
        "for label in all_labels:\n",
        "    if isinstance(label, bool):\n",
        "        binary_labels.append(1 if label else 0)\n",
        "    elif isinstance(label, (int, float)):\n",
        "        binary_labels.append(1 if label > 0 else 0)\n",
        "    else:\n",
        "        binary_labels.append(0)  # Default to non-hate for unknown labels\n",
        "\n",
        "print(f\"Binary label distribution: {np.bincount(binary_labels)}\")\n",
        "print(f\"Hate speech percentage: {np.mean(binary_labels):.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create stratified train/val/test split (80/10/10)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    all_texts, binary_labels, test_size=0.1, random_state=42, stratify=binary_labels\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.111, random_state=42, stratify=y_temp  # 0.111 * 0.9 = 0.1\n",
        ")\n",
        "\n",
        "print(f\"Train set: {len(X_train)} samples\")\n",
        "print(f\"Validation set: {len(X_val)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "print(f\"Train label distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Val label distribution: {np.bincount(y_val)}\")\n",
        "print(f\"Test label distribution: {np.bincount(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Baseline Models Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear any old function references and ensure we use the ensemble version\n",
        "try:\n",
        "    del train_baseline_models\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "# Force reload the ensemble module to get the latest version\n",
        "import importlib\n",
        "import baseline_ensemble\n",
        "importlib.reload(baseline_ensemble)\n",
        "from baseline_ensemble import train_ensemble_baseline_models\n",
        "\n",
        "print(\"‚úÖ Ensemble module reloaded with SVC support for soft voting\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import training modules\n",
        "from baseline_ensemble import train_ensemble_baseline_models\n",
        "from transformer_training_optimized import train_multilingual_transformer_optimized\n",
        "from model_export import save_baseline_model, save_huggingface_model, save_torchscript_model, save_onnx_model\n",
        "\n",
        "# This cell is replaced by the ensemble training cell below\n",
        "# Skip this cell and run the ensemble training cell instead\n",
        "print(\"‚ö†Ô∏è  This cell is deprecated. Please run the ensemble training cell below.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run ensemble baseline training\n",
        "print(\"üöÄ Starting Ensemble Baseline Training...\")\n",
        "print(\"This will train Logistic Regression, Linear SVC, Random Forest, and their ensemble\")\n",
        "print(\"Expected time: 20-30 minutes\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_baseline_model, baseline_test_results, cm_baseline, individual_results, ensemble_results = train_ensemble_baseline_models(\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Ensemble baseline training completed!\")\n",
        "print(f\"üèÜ Best baseline F1 score: {baseline_test_results['f1_macro']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train ensemble baseline models\n",
        "print(\"Training ensemble baseline models...\")\n",
        "best_baseline_model, baseline_test_results, cm_baseline, individual_results, ensemble_results = train_ensemble_baseline_models(\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the fixed model export module with opset 14 support\n",
        "import importlib\n",
        "import model_export\n",
        "importlib.reload(model_export)\n",
        "from model_export import save_baseline_model, save_huggingface_model, save_torchscript_model, save_onnx_model\n",
        "\n",
        "print(\"‚úÖ Model export module reloaded with opset 14 support for scaled_dot_product_attention\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transformer Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download exported transformer models to your device\n",
        "print(\"üöÄ Preparing transformer models for download...\")\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "def create_transformer_models_zip():\n",
        "    \"\"\"Create a ZIP file with only exported transformer models for download\"\"\"\n",
        "    \n",
        "    # Create models directory structure\n",
        "    models_dir = \"transformer_models\"\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    \n",
        "    # Copy transformer HuggingFace model\n",
        "    if os.path.exists(\"models/transformer/hf_model\"):\n",
        "        shutil.copytree(\"models/transformer/hf_model\", f\"{models_dir}/hf_model\", dirs_exist_ok=True)\n",
        "        print(\"‚úÖ HuggingFace model copied\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  HuggingFace model not found\")\n",
        "    \n",
        "    # Copy ONNX model\n",
        "    if os.path.exists(\"models/transformer/onnx/model.onnx\"):\n",
        "        os.makedirs(f\"{models_dir}/onnx\", exist_ok=True)\n",
        "        shutil.copy2(\"models/transformer/onnx/model.onnx\", f\"{models_dir}/onnx/model.onnx\")\n",
        "        print(\"‚úÖ ONNX model copied\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  ONNX model not found\")\n",
        "    \n",
        "    # Copy TorchScript model\n",
        "    if os.path.exists(\"models/transformer/torchscript/model.pt\"):\n",
        "        os.makedirs(f\"{models_dir}/torchscript\", exist_ok=True)\n",
        "        shutil.copy2(\"models/transformer/torchscript/model.pt\", f\"{models_dir}/torchscript/model.pt\")\n",
        "        print(\"‚úÖ TorchScript model copied\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  TorchScript model not found\")\n",
        "    \n",
        "    # Create ZIP file\n",
        "    zip_filename = \"transformer_models.zip\"\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(models_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, models_dir)\n",
        "                zipf.write(file_path, arcname)\n",
        "    \n",
        "    print(f\"\\nüéâ Transformer models ZIP created: {zip_filename}\")\n",
        "    print(f\"üìä File size: {os.path.getsize(zip_filename) / (1024*1024):.1f} MB\")\n",
        "    \n",
        "    return zip_filename\n",
        "\n",
        "# Create and download transformer models\n",
        "zip_filename = create_transformer_models_zip()\n",
        "\n",
        "print(f\"\\nüì• Downloading {zip_filename} to your device...\")\n",
        "files.download(zip_filename)\n",
        "\n",
        "print(\"\\n‚úÖ Download complete!\")\n",
        "print(\"\\nüìã What you downloaded:\")\n",
        "print(\"   ‚Ä¢ HuggingFace transformer model (config.json, model.safetensors, etc.)\")\n",
        "print(\"   ‚Ä¢ ONNX model (model.onnx)\")\n",
        "print(\"   ‚Ä¢ TorchScript model (model.pt)\")\n",
        "\n",
        "print(\"\\nüéØ Next steps:\")\n",
        "print(\"   1. Extract the ZIP file on your device\")\n",
        "print(\"   2. Copy models to your API directory:\")\n",
        "print(\"      - Copy hf_model/ to api/models/transformer/\")\n",
        "print(\"      - Copy onnx/model.onnx to api/models/transformer/onnx/\")\n",
        "print(\"   3. Start your API server\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train multilingual transformer model (OPTIMIZED VERSION)\n",
        "print(\"Training multilingual transformer model for Arabic, Turkish, and English (OPTIMIZED)...\")\n",
        "model, tokenizer, transformer_test_results, cm_transformer = train_multilingual_transformer_optimized(\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test,\n",
        "    model_name=\"xlm-roberta-base\",\n",
        "    max_length=128,\n",
        "    batch_size=16,  # Larger batch size for GPU\n",
        "    learning_rate=1e-5,  # Lower learning rate for better convergence\n",
        "    num_epochs=5  # More epochs for better learning\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Export and Saving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all models\n",
        "print(\"Saving models...\")\n",
        "\n",
        "# Save baseline model\n",
        "save_baseline_model(best_baseline_model, \"models/baseline/baseline_model.pkl\")\n",
        "\n",
        "# Save HuggingFace model\n",
        "save_huggingface_model(model, tokenizer, \"models/transformer/hf_model\")\n",
        "\n",
        "# Save TorchScript model\n",
        "save_torchscript_model(model, tokenizer, \"models/transformer/torchscript/model.pt\", max_length=128)\n",
        "\n",
        "# Save ONNX model\n",
        "save_onnx_model(model, \"models/transformer/onnx/model.onnx\", max_length=128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed results including ensemble comparison\n",
        "import pandas as pd\n",
        "\n",
        "# Combine all results\n",
        "all_results = [baseline_test_results, transformer_test_results]\n",
        "\n",
        "# Add individual model results for comparison\n",
        "for model_name, results in individual_results.items():\n",
        "    results['model'] = f'Baseline-{model_name}'\n",
        "    all_results.append(results)\n",
        "\n",
        "# Add ensemble results\n",
        "ensemble_results['model'] = 'Baseline-Ensemble'\n",
        "all_results.append(ensemble_results)\n",
        "\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "# Save comprehensive results\n",
        "results_df.to_csv('results/metrics_comprehensive.csv', index=False)\n",
        "print(\"üìä Comprehensive results saved to results/metrics_comprehensive.csv\")\n",
        "\n",
        "# Display results comparison\n",
        "print(\"\\nüìä Model Performance Comparison:\")\n",
        "print(\"=\" * 60)\n",
        "for _, row in results_df.iterrows():\n",
        "    print(f\"{row['model']:<25} F1-macro: {row['f1_macro']:.4f} | F1-weighted: {row['f1_weighted']:.4f}\")\n",
        "\n",
        "print(\"\\nüèÜ Best performing models:\")\n",
        "best_individual = results_df[results_df['model'].str.contains('Baseline-') & ~results_df['model'].str.contains('Ensemble')].nlargest(1, 'f1_macro')\n",
        "best_ensemble = results_df[results_df['model'].str.contains('Ensemble')].nlargest(1, 'f1_macro')\n",
        "best_transformer = results_df[results_df['model'].str.contains('XLM-RoBERTa')].nlargest(1, 'f1_macro')\n",
        "\n",
        "print(f\"Best Individual Baseline: {best_individual.iloc[0]['model']} (F1: {best_individual.iloc[0]['f1_macro']:.4f})\")\n",
        "print(f\"Best Ensemble Baseline: {best_ensemble.iloc[0]['model']} (F1: {best_ensemble.iloc[0]['f1_macro']:.4f})\")\n",
        "print(f\"Best Transformer: {best_transformer.iloc[0]['model']} (F1: {best_transformer.iloc[0]['f1_macro']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results and metrics\n",
        "import pandas as pd\n",
        "\n",
        "# Combine all results\n",
        "all_results = [baseline_test_results, transformer_test_results]\n",
        "results_df = pd.DataFrame(all_results)\n",
        "\n",
        "# Save to CSV\n",
        "os.makedirs('results', exist_ok=True)\n",
        "results_df.to_csv('results/metrics.csv', index=False)\n",
        "print(\"Results saved: results/metrics.csv\")\n",
        "print(\"\\nResults Summary:\")\n",
        "print(results_df.round(4))\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n=== Training Complete! ===\")\n",
        "print(\"\\nSaved artifacts:\")\n",
        "print(\"üìÅ models/baseline/baseline_model.pkl\")\n",
        "print(\"üìÅ models/transformer/hf_model/ (HuggingFace model)\")\n",
        "print(\"üìÅ models/transformer/torchscript/model.pt\")\n",
        "print(\"üìÅ models/transformer/onnx/model.onnx\")\n",
        "print(\"üìÅ results/metrics.csv\")\n",
        "print(\"üìÅ results/confusion_baseline.png\")\n",
        "print(\"üìÅ results/confusion_transformer.png\")\n",
        "\n",
        "print(\"\\nModel Performance Summary:\")\n",
        "print(f\"Best Baseline: F1-macro = {baseline_test_results['f1_macro']:.4f}\")\n",
        "print(f\"Transformer (XLM-RoBERTa): F1-macro = {transformer_test_results['f1_macro']:.4f}\")\n",
        "\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Download the models folder to your local machine\")\n",
        "print(\"2. Set up the FastAPI server using the exported models\")\n",
        "print(\"3. Build and deploy the Android app\")\n",
        "print(\"4. Run the report notebook for detailed analysis\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
