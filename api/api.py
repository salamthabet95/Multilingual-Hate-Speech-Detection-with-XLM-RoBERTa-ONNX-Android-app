#!/usr/bin/env python3
"""
Simple ONNX API server without full transformers dependency
"""

import http.server
import socketserver
import json
import os
import numpy as np
import onnxruntime as ort
import re

# Global variables
onnx_session = None

def load_onnx_model():
    """Load the ONNX model"""
    global onnx_session
    
    try:
        onnx_path = "models/transformer/onnx/model.onnx"
        
        if not os.path.exists(onnx_path):
            raise FileNotFoundError(f"ONNX model not found at {onnx_path}")
        
        print(f"Loading ONNX model from {onnx_path}...")
        onnx_session = ort.InferenceSession(onnx_path)
        
        print("‚úÖ ONNX model loaded successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå Error loading ONNX model: {e}")
        return False

def simple_tokenize(text, max_length=128):
    """Simple tokenization without transformers library"""
    # Basic tokenization - split by spaces and common punctuation
    import re
    
    # Clean text
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    words = text.split()
    
    # Simple word-to-id mapping (this is a basic approach)
    # In a real implementation, you'd use the actual tokenizer
    word_to_id = {}
    current_id = 1
    
    # Create simple vocabulary
    for word in words[:max_length]:
        if word not in word_to_id:
            word_to_id[word] = current_id
            current_id += 1
    
    # Convert to IDs
    input_ids = [word_to_id.get(word, 0) for word in words[:max_length]]
    
    # Pad to max_length
    while len(input_ids) < max_length:
        input_ids.append(0)
    
    # Create attention mask
    attention_mask = [1 if id != 0 else 0 for id in input_ids]
    
    return np.array([input_ids]), np.array([attention_mask])

def predict_hate_speech(text: str) -> dict:
    """Predict hate speech using the ONNX model"""
    global onnx_session
    
    if onnx_session is None:
        raise Exception("Model not loaded")
    
    try:
        # Simple tokenization
        input_ids, attention_mask = simple_tokenize(text)
        
        # Prepare inputs for ONNX
        onnx_inputs = {
            "input_ids": input_ids.astype(np.int64),
            "attention_mask": attention_mask.astype(np.int64)
        }
        
        # Run inference
        outputs = onnx_session.run(None, onnx_inputs)
        logits = outputs[0]
        
        # Get prediction and confidence
        probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
        prediction = np.argmax(probabilities, axis=1)[0]
        confidence = probabilities[0][prediction]
        
        return {
            "prediction": int(prediction),
            "confidence": round(float(confidence), 3),
            "model_used": "transformer"
        }
        
    except Exception as e:
        raise Exception(f"Prediction error: {str(e)}")

class APIHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/health':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            response = {
                "status": "healthy",
                "transformer_loaded": onnx_session is not None,
                "model_type": "ONNX Transformer (Simple)",
                "message": "API is running with real transformer model"
            }
            self.wfile.write(json.dumps(response).encode())
            
        elif self.path == '/':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            response = {
                "message": "Multilingual Hate Speech Detection API",
                "version": "1.0.0",
                "model_loaded": onnx_session is not None,
                "model_type": "Real Trained Transformer (ONNX Simple)",
                "endpoints": ["/health", "/predict"],
                "status": "running"
            }
            self.wfile.write(json.dumps(response).encode())
            
        else:
            self.send_response(404)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            response = {"error": "Not found"}
            self.wfile.write(json.dumps(response).encode())
    
    def do_POST(self):
        if self.path == '/predict':
            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            
            try:
                data = json.loads(post_data.decode('utf-8'))
                text = data.get('text', '')
                
                if not text.strip():
                    raise Exception("Empty text provided")
                
                # Use the real ONNX transformer model
                result = predict_hate_speech(text)
                
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.send_header('Access-Control-Allow-Origin', '*')
                self.end_headers()
                self.wfile.write(json.dumps(result).encode())
                
            except Exception as e:
                self.send_response(400)
                self.send_header('Content-type', 'application/json')
                self.send_header('Access-Control-Allow-Origin', '*')
                self.end_headers()
                response = {"error": str(e)}
                self.wfile.write(json.dumps(response).encode())
        else:
            self.send_response(404)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            response = {"error": "Not found"}
            self.wfile.write(json.dumps(response).encode())

if __name__ == "__main__":
    PORT = 8002
    
    print("üöÄ Starting REAL Transformer API Server (ONNX Simple)...")
    print("üì° Loading your trained model...")
    
    # Load the model
    success = load_onnx_model()
    if not success:
        print("‚ùå Failed to load ONNX model!")
        print("üí° Make sure the ONNX model file exists")
        exit(1)
    
    print(f"üì° Server will run on: http://localhost:{PORT}")
    print("üìã Available endpoints:")
    print("  GET  /health - Health check")
    print("  GET  / - API information")
    print("  POST /predict - Predict hate speech")
    print("\nüì± For Android app:")
    print("  - Use http://10.0.2.2:8000/ (Android emulator)")
    print("  - Use http://192.168.1.108:8000/ (real device)")
    print("\nüéØ Using REAL trained transformer model (ONNX)!")
    print("üõë Press Ctrl+C to stop the server")
    
    with socketserver.TCPServer(("", PORT), APIHandler) as httpd:
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("\nüëã Server stopped!")
